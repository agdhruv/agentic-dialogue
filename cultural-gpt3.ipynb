{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import utils\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import constants\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I tried to see if assigning relevant personas to an LLM helps it perform better on cultural benchmarks.\n",
    "- The first experiment is baseline -- no persona.\n",
    "- In one experiment, I downloaded 200k personas from PersonaHub, computed a embedding for all of them, and the took the persona with the highest cosine similarity for each question.\n",
    "- In another one, I first prompted an LLM (GPT3.5) to generate a persona appropriate to answer the question, and then used that persona (personaGen).\n",
    "- Also tried a \"discussion among agents\" approach, but that didn't work too well, so I didn't run it on the entire benchmark dataset.\n",
    "\n",
    "I didn't see significant improvement in the performance on CulturalBench, so didn't proceed in this direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load culturalbench dataset + question embeddings\n",
    "ds = load_dataset(\"kellycyy/CulturalBench\", \"CulturalBench-Easy\", split='test')\n",
    "df = ds.to_pandas()\n",
    "\n",
    "with open('data/question_embeddings.json') as f:\n",
    "  question_embeddings = json.load(f)\n",
    "question_embeddings = torch.tensor(question_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load persona dataset + persona embeddings\n",
    "dfp = pd.read_json(\"hf://datasets/proj-persona/PersonaHub/persona.jsonl\", lines=True)\n",
    "\n",
    "# load \n",
    "with open('data/persona_embeddings.json') as f:\n",
    "  persona_embeddings = json.load(f)\n",
    "persona_embeddings = torch.tensor(persona_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between each question and persona -- need to do this efficiently as the dataset is large\n",
    "\n",
    "# Normalize embeddings\n",
    "question_embeddings = question_embeddings / question_embeddings.norm(dim=1, keepdim=True)\n",
    "persona_embeddings = persona_embeddings / persona_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarities = torch.matmul(question_embeddings, persona_embeddings.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "  \"To answer the following multiple-choice question, you should choose one option only among A, B, C, D.\\n\"\n",
    "  \"Instruction: You must select one option among A, B, C, D. Do not output any other things.\\n\"\n",
    "  \"{}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In Mexican culture, what is a characteristic not commonly found in employment ads?\n",
      "A. Specification of an age range for applicants\n",
      "B. Encouragement for only online applications\n",
      "C. A requirement for a video introduction from the applicant\n",
      "D. Offering job positions without specific qualifications.\n"
     ]
    }
   ],
   "source": [
    "question_idx = 245\n",
    "question_string = utils.format_question_culturalbench_easy(ds[question_idx])\n",
    "print(question_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt3(client, question_string):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt.format(question_string)\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    response_format={\n",
    "      \"type\": \"text\"\n",
    "    },\n",
    "    temperature=1,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping question 0\n",
      "Skipping question 1\n",
      "Skipping question 2\n",
      "Skipping question 3\n",
      "Skipping question 4\n",
      "Skipping question 5\n",
      "Skipping question 6\n",
      "Skipping question 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1227/1227 [10:03<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "for i in tqdm(range(len(ds))):\n",
    "  filename = f'results/gpt3.5/question_{i}.json'\n",
    "\n",
    "  if os.path.exists(filename):\n",
    "    print(f\"Skipping question {i}\")\n",
    "    continue\n",
    "  \n",
    "  # print(f\"Processing question {i}\")\n",
    "  question_string = utils.format_question_culturalbench_easy(ds[i])\n",
    "  response = ask_gpt3(client, question_string)\n",
    "\n",
    "  with open(filename, 'w') as f:\n",
    "    f.write(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Persona from Personahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "  \"To answer the following multiple-choice question, you should choose one option only among A, B, C, D.\\n\"\n",
    "  \"Instruction: You must select one option among A, B, C, D. Do not output any other things.\\n\"\n",
    "  \"{}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt3_with_persona(client, question_string, persona_string):\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": f\"You are: {persona_string}\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": prompt.format(question_string)\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    response_format={\n",
    "      \"type\": \"text\"\n",
    "    },\n",
    "    temperature=1,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1227 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping question 0\n",
      "Skipping question 1\n",
      "Skipping question 2\n",
      "Skipping question 3\n",
      "Skipping question 4\n",
      "Skipping question 5\n",
      "Skipping question 6\n",
      "Skipping question 7\n",
      "Skipping question 8\n",
      "Skipping question 9\n",
      "Skipping question 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1227/1227 [10:55<00:00,  1.87it/s] \n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "for i in tqdm(range(len(ds))):\n",
    "  filename = f'results/gpt3.5-persona/question_{i}.json'\n",
    "\n",
    "  if os.path.exists(filename):\n",
    "    print(f\"Skipping question {i}\")\n",
    "    continue\n",
    "  \n",
    "  # print(f\"Processing question {i}\")\n",
    "  question_string = utils.format_question_culturalbench_easy(ds[i])\n",
    "  persona_string = dfp.iloc[torch.argmax(cosine_similarities[i]).item()]['persona']\n",
    "  response = ask_gpt3_with_persona(client, question_string, persona_string)\n",
    "\n",
    "  with open(filename, 'w') as f:\n",
    "    f.write(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_personas(client, question_string):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"You are asked a cultural question. Based on the question, assign four persona descriptions based on who you think would be the right people to ask such a question. Ensure each persona description is less than 50 characters in length.\\nOutput in JSON format: {\\\"persona1\\\": \\\"description1\\\", \\\"persona2\\\": \\\"description2\\\", \\\"persona3\\\": \\\"description3\\\", \\\"persona4\\\": \\\"description4\\\"}\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": question_string\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    response_format={\n",
    "      \"type\": \"json_object\"\n",
    "    },\n",
    "    temperature=1,\n",
    "    max_tokens=2048,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "\n",
    "  return response\n",
    "\n",
    "for i in tqdm(range(len(ds))):\n",
    "  filename = f'data/generated_personas/question_{i}.json'\n",
    "\n",
    "  if os.path.exists(filename):\n",
    "    print(f\"Skipping question {i}\")\n",
    "    continue\n",
    "  \n",
    "  question_string = ds[i]['prompt_question']\n",
    "  response = create_personas(client, question_string)\n",
    "\n",
    "  with open(filename, 'w') as f:\n",
    "    f.write(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1227/1227 [00:00<00:00, 7139.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load generated personas\n",
    "\n",
    "generated_personas = []\n",
    "for i in tqdm(range(len(ds))):\n",
    "  filename = f'data/generated_personas/question_{i}.json'\n",
    "  with open(filename) as f:\n",
    "    response = json.load(f)\n",
    "    personas = json.loads(response['choices'][0]['message']['content'])\n",
    "    generated_personas.append(personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1227/1227 [09:49<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "for i in tqdm(range(len(ds))):\n",
    "  filename = f'results/gpt3.5-personaGen/question_{i}.json'\n",
    "\n",
    "  if os.path.exists(filename):\n",
    "    print(f\"Skipping question {i}\")\n",
    "    continue\n",
    "  \n",
    "  question_string = utils.format_question_culturalbench_easy(ds[i])\n",
    "  persona_string = generated_personas[i]['persona1']\n",
    "  response = ask_gpt3_with_persona(client, question_string, persona_string)\n",
    "\n",
    "  with open(filename, 'w') as f:\n",
    "    f.write(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Amongst Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agdhruv/opt/anaconda3/envs/caai/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "from autogen import ConversableAgent, GroupChat, GroupChatManager\n",
    "from autogen.agentchat.contrib.society_of_mind_agent import SocietyOfMindAgent\n",
    "import utils\n",
    "import os\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In Mexican culture, what is a characteristic not commonly found in employment ads?\n",
      "A. Specification of an age range for applicants\n",
      "B. Encouragement for only online applications\n",
      "C. A requirement for a video introduction from the applicant\n",
      "D. Offering job positions without specific qualifications.\n",
      "{'persona1': 'HR manager in Mexico', 'persona2': 'Mexican cultural historian', 'persona3': 'Employment lawyer in Mexico', 'persona4': 'Mexican job applicant'}\n"
     ]
    }
   ],
   "source": [
    "question_idx = 245\n",
    "question_string = utils.format_question_culturalbench_easy(ds[question_idx])\n",
    "personas = generated_personas[question_idx]\n",
    "print(question_string)\n",
    "print(personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_agents = []\n",
    "\n",
    "for persona in personas.values():\n",
    "  persona_label = persona.replace(' ', '_').replace('/', '_or_')\n",
    "  agent = ConversableAgent(\n",
    "    name=persona_label,\n",
    "    description=f\"A {persona}\",\n",
    "    system_message=f\"You are: {persona}. You will be asked a question. You initially support options Stick to your opinion unless presented with compelling arguments.\",\n",
    "    llm_config={\"config_list\": constants.config_list},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg.get(\"content\")\n",
    "  )\n",
    "  discussion_agents.append(agent)\n",
    "\n",
    "# Define the admin agent\n",
    "admin = ConversableAgent(\n",
    "  name=\"Admin\",\n",
    "  system_message=(\"You are a neutral discussion administrator. Your role is to keep the agents on track.\"\n",
    "                  \"Once ALL agents agree on a response, print the answer and say TERMINATE.\"\n",
    "                  \"If there is disagreement or insufficient clarity in the responses, summarize the points of disagreement and allow agents to continue discussing.\"\n",
    "                  \"Remember, your goal is to facilitate agreement and terminate only when consensus is reached among ALL agents.\"),\n",
    "  description=\"A neutral discussion administrator.\",\n",
    "  llm_config={\"config_list\": constants.config_list},\n",
    "  is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg.get(\"content\"),\n",
    "  human_input_mode=\"NEVER\"\n",
    ")\n",
    "discussion_agents.append(admin)\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=discussion_agents,\n",
    "    messages=[],\n",
    "    max_round=15,\n",
    "    send_introductions=True,\n",
    "    speaker_selection_method=\"round_robin\"\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config={\"config_list\": constants.config_list},\n",
    "    human_input_mode=\"NEVER\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_agents = []\n",
    "\n",
    "# Define the admin agent\n",
    "admin = ConversableAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"You are a neutral disscussion administrator. Your role is to pose the initial question and encourage discussion among the agents. Once a consensus is reached, print the answer and say TERMINATE.\",\n",
    "    description=\"A neutral discussion administrator.\",\n",
    "    llm_config={\"config_list\": constants.config_list},\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg.get(\"content\"),\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "discussion_agents.append(admin)\n",
    "\n",
    "for i in range(4):\n",
    "    persona = list(personas.values())[i]\n",
    "    persona_label = persona.replace(' ', '_').replace('/', '_or_')\n",
    "    agent = ConversableAgent(\n",
    "        name=persona_label,\n",
    "        system_message=f\"You are asked a question. You support option {chr(65 + i)}. Discuss with other agents to reach a consensus on one of the options.\",\n",
    "        description=persona,\n",
    "        llm_config={\"config_list\": constants.config_list},\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg.get(\"content\")\n",
    "    )\n",
    "    discussion_agents.append(agent)\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=discussion_agents,\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    send_introductions=True,\n",
    "    speaker_selection_method=\"auto\"\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config={\"config_list\": constants.config_list},\n",
    "    human_input_mode=\"NEVER\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to society_of_mind):\n",
      "\n",
      "Question: In Mexican culture, what is a characteristic not commonly found in employment ads?\n",
      "A. Specification of an age range for applicants\n",
      "B. Encouragement for only online applications\n",
      "C. A requirement for a video introduction from the applicant\n",
      "D. Offering job positions without specific qualifications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msociety_of_mind\u001b[0m (to chat_manager):\n",
      "\n",
      "Question: In Mexican culture, what is a characteristic not commonly found in employment ads?\n",
      "A. Specification of an age range for applicants\n",
      "B. Encouragement for only online applications\n",
      "C. A requirement for a video introduction from the applicant\n",
      "D. Offering job positions without specific qualifications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: HR_manager_in_Mexico\n",
      "\u001b[0m\n",
      "\u001b[33mHR_manager_in_Mexico\u001b[0m (to chat_manager):\n",
      "\n",
      "HR_manager_in_Mexico: In Mexican culture, option A, specifying an age range for applicants, is not commonly found in employment ads. Age discrimination is a sensitive issue in Mexico, and job postings typically avoid mentioning age requirements to promote inclusivity and diversity in the workplace.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Mexican_cultural_historian\n",
      "\u001b[0m\n",
      "\u001b[33mMexican_cultural_historian\u001b[0m (to chat_manager):\n",
      "\n",
      "Mexican_cultural_historian: I agree with HR_manager_in_Mexico. Specifying an age range for applicants is not a common practice in employment ads in Mexican culture. Age discrimination is prohibited by law in Mexico, and most companies strive to promote equal opportunities for all individuals in the job market. It is considered more in line with Mexican cultural values to focus on the skills and qualifications of applicants rather than their age.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Employment_lawyer_in_Mexico\n",
      "\u001b[0m\n",
      "\u001b[33mEmployment_lawyer_in_Mexico\u001b[0m (to chat_manager):\n",
      "\n",
      "Employment_lawyer_in_Mexico: I concur with my colleagues. The specification of an age range for applicants is not commonly found in employment ads in Mexican culture due to the strict legal prohibition against age discrimination in the country. Employers are required to focus on the skills and qualifications of applicants rather than their age to ensure equal opportunities for all individuals in the job market.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Mexican_job_applicant\n",
      "\u001b[0m\n",
      "\u001b[33mMexican_job_applicant\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the inputs from HR_manager_in_Mexico, Mexican_cultural_historian, and Employment_lawyer_in_Mexico, it is evident that specifying an age range for applicants is not a common practice in Mexican culture due to legal regulations and cultural values promoting inclusivity and equal opportunities in the job market. Therefore, it is safe to say that option A, Specification of an age range for applicants, is the characteristic not commonly found in employment ads in Mexican culture.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "The characteristic not commonly found in employment ads in Mexican culture is:\n",
      "\n",
      "A. Specification of an age range for applicants\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: HR_manager_in_Mexico\n",
      "\u001b[0m\n",
      "\u001b[33msociety_of_mind\u001b[0m (to user_proxy):\n",
      "\n",
      "{\n",
      "  \"disagreement\": false,\n",
      "  \"answer\": \"A\"\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "society_of_mind_agent = SocietyOfMindAgent(\n",
    "  \"society_of_mind\",\n",
    "  human_input_mode=\"NEVER\",\n",
    "  response_preparer=constants.SUMMARY_PROMPT_MULTIPLE_CHOICE,\n",
    "  chat_manager=manager,\n",
    "  llm_config={\"config_list\": constants.config_list},\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "  \"user_proxy\",\n",
    "  human_input_mode=\"NEVER\",\n",
    "  code_execution_config=False,\n",
    "  is_termination_msg=lambda x: True,\n",
    ")\n",
    "\n",
    "result = user_proxy.initiate_chat(society_of_mind_agent, message=question_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1227/1227 [00:00<00:00, 6995.80it/s]\n",
      "100%|██████████| 1227/1227 [00:00<00:00, 6595.68it/s]\n",
      "100%|██████████| 1227/1227 [00:00<00:00, 8899.95it/s]\n"
     ]
    }
   ],
   "source": [
    "methods = ['gpt3.5', 'gpt3.5-personahub', 'gpt3.5-personaGen']\n",
    "\n",
    "# Load results and analyze\n",
    "results = {}\n",
    "for method in methods:\n",
    "  results[method] = {}\n",
    "  for i in tqdm(range(len(ds))):\n",
    "    fname = f'results/{method}/question_{i}.json'\n",
    "    with open(fname, 'r') as f:\n",
    "      response = json.load(f)\n",
    "    results[method][i] = response['choices'][0]['message']['content']\n",
    "\n",
    "  df[method] = [r.strip()[0] for r in results[method].values()]\n",
    "  df[f'{method}_correct'] = df[method] == df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: gpt3.5\n",
      "Accuracy: 0.6960065199674002\n",
      "Method: gpt3.5-personahub\n",
      "Accuracy: 0.6797066014669927\n",
      "Method: gpt3.5-personaGen\n",
      "Accuracy: 0.6976365118174409\n"
     ]
    }
   ],
   "source": [
    "for method in methods:\n",
    "  print(f\"Method: {method}\")\n",
    "  print(f\"Accuracy: {df[f'{method}_correct'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
